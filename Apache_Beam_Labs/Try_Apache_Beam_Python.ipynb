{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNKIMlEDZ_Vw"
   },
   "source": [
    "# Try Apache Beam - Python\n",
    "\n",
    "In this notebook, we set up your development environment and work through a simple example using the [DirectRunner](https://beam.apache.org/documentation/runners/direct/). You can explore other runners with the [Beam Capatibility Matrix](https://beam.apache.org/documentation/runners/capability-matrix/).\n",
    "\n",
    "To navigate through different sections, use the table of contents. From **View**  drop-down list, select **Table of contents**.\n",
    "\n",
    "To run a code cell, you can click the **Run cell** button at the top left of the cell, or by select it and press **`Shift+Enter`**. Try modifying a code cell and re-running it to see what happens.\n",
    "\n",
    "To learn more about Colab, see [Welcome to Colaboratory!](https://colab.sandbox.google.com/notebooks/welcome.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fz6KSQ13_3Rr"
   },
   "source": [
    "# Setup\n",
    "\n",
    "First, you need to set up your environment, which includes installing `apache-beam` and downloading a text file from Cloud Storage to your local file system. We are using this file to test your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import re\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "GOOk81Jj_yUy",
    "outputId": "d283dfb2-4f51-4fec-816b-f57b0cb9b71c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> mkdir -p data\n",
      "\n",
      ">> gsutil cp gs://dataflow-samples/shakespeare/kinglear.txt data/\n",
      "Copying gs://dataflow-samples/shakespeare/kinglear.txt...\n",
      "/ [1 files][153.6 KiB/153.6 KiB]                                                \n",
      "Operation completed over 1 objects/153.6 KiB.                                    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run and print a shell command.\n",
    "def run(cmd):\n",
    "  print('>> {}'.format(cmd))\n",
    "  !{cmd}\n",
    "  print('')\n",
    "\n",
    "# Install apache-beam.\n",
    "# run('pip install --quiet apache-beam')\n",
    "\n",
    "# Copy the input file into the local file system.\n",
    "run('mkdir -p data')\n",
    "run('gsutil cp gs://dataflow-samples/shakespeare/kinglear.txt data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-HubCrk-h_G"
   },
   "source": [
    "# Word count with comments\n",
    "\n",
    "Below is mostly the same code as above, but with comments explaining every line in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1173
    },
    "id": "x_D7sxUHFzUp",
    "outputId": "44c926df-aa4a-4bea-9247-27c7cb537717"
   },
   "outputs": [],
   "source": [
    "inputs_pattern = 'data/*'\n",
    "outputs_prefix = 'outputs/part'\n",
    "\n",
    "# Running locally in the DirectRunner.\n",
    "with beam.Pipeline() as pipeline:\n",
    "  # Store the word counts in a PCollection.\n",
    "  # Each element is a tuple of (word, count) of types (str, int).\n",
    "  word_counts = (\n",
    "      # The input PCollection is an empty pipeline.\n",
    "      pipeline\n",
    "\n",
    "      # Read lines from a text file.\n",
    "      | 'Read lines' >> beam.io.ReadFromText(inputs_pattern)\n",
    "      # Element type: str - text line\n",
    "\n",
    "      # Use a regular expression to iterate over all words in the line.\n",
    "      # FlatMap will yield an element for every element in an iterable.\n",
    "      | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line))\n",
    "      # Element type: str - word\n",
    "      \n",
    "      ##### MODIFICATION 1 #####\n",
    "      \n",
    "      # Modified by Rohan: Convert all words to lowercase to ensure that the same words with different cases are counted together. \n",
    "      # For example, 'The' and 'the' will be treated as the same word 'the'. Similary 'KING', 'King' and 'king' will be treated\n",
    "      # as the same word 'king'.\n",
    "      | 'Lowercase words' >> beam.Map(lambda word: word.lower())\n",
    "\n",
    "      # Create key-value pairs where the value is 1, this way we can group by\n",
    "      # the same word while adding those 1s and get the counts for every word.\n",
    "      | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
    "      # Element type: (str, int) - key: word, value: 1\n",
    "\n",
    "      # Group by key while combining the value using the sum() function.\n",
    "      | 'Group and sum' >> beam.CombinePerKey(sum)\n",
    "      # Element type: (str, int) - key: word, value: counts\n",
    "  )\n",
    "\n",
    "  # We can process a PCollection through other pipelines too.\n",
    "  (\n",
    "      # The input PCollection is the word_counts created from the previous step.\n",
    "      word_counts\n",
    "\n",
    "      # Format the results into a string so we can write them to a file.\n",
    "      | 'Format results' >> beam.Map(lambda word_count: str(word_count))\n",
    "      # Element type: str - text line\n",
    "\n",
    "      # Finally, write the results to a file.\n",
    "      | 'Write results' >> beam.io.WriteToText(outputs_prefix)\n",
    "  )\n",
    "\n",
    "# Sample the first 20 results, remember there are no ordering guarantees.\n",
    "run('head -n 200 {}-00000-of-*'.format(outputs_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "#### Using SparkRunner (Executing on local Spark engine)\n",
    "#### This method is designed to cater to real workloads since it involves full Spark parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['--spark_master=local[*]']\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 8 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> cat outputs_spark/part-* > outputs_spark/all_results.txt\n",
      "\n",
      ">> head -n 200 outputs_spark/part-00000-of-*\n",
      "('few', 1)\n",
      "('lecherous', 1)\n",
      "('adieu', 1)\n",
      "('such', 44)\n",
      "('persuade', 2)\n",
      "('bred', 2)\n",
      "('hourly', 3)\n",
      "(\"fellow's\", 1)\n",
      "('guardians', 1)\n",
      "('heart', 49)\n",
      "('cur', 2)\n",
      "('terrible', 2)\n",
      "('worth', 9)\n",
      "('mire', 1)\n",
      "('physician', 1)\n",
      "('spirit', 6)\n",
      "('friendly', 2)\n",
      "('wish', 2)\n",
      "('owest', 2)\n",
      "('longer', 2)\n",
      "('kinder', 1)\n",
      "('poorest', 2)\n",
      "('kin', 1)\n",
      "('fleshment', 1)\n",
      "(\"'parel\", 1)\n",
      "('thanks', 2)\n",
      "('sing', 2)\n",
      "('matter', 15)\n",
      "('its', 1)\n",
      "('party', 4)\n",
      "('banishment', 3)\n",
      "('liege', 2)\n",
      "('lightning', 1)\n",
      "('speaks', 3)\n",
      "('lacks', 1)\n",
      "('soliciting', 1)\n",
      "('songs', 1)\n",
      "('attempting', 1)\n",
      "('moonshine', 1)\n",
      "('shows', 3)\n",
      "(\"talk'd\", 1)\n",
      "('deadly', 2)\n",
      "('masts', 1)\n",
      "('steeples', 1)\n",
      "('sixth', 1)\n",
      "('suits', 1)\n",
      "('miles', 1)\n",
      "('custom', 1)\n",
      "('borest', 1)\n",
      "('bondage', 1)\n",
      "('rotundity', 1)\n",
      "('turns', 3)\n",
      "('throughly', 1)\n",
      "('morning', 1)\n",
      "('sennet', 1)\n",
      "('light', 5)\n",
      "('broils', 1)\n",
      "('beaks', 1)\n",
      "(\"tom's\", 7)\n",
      "('contriving', 1)\n",
      "(\"unburthen'd\", 1)\n",
      "('che', 1)\n",
      "('stratagem', 1)\n",
      "('three', 6)\n",
      "('serving', 1)\n",
      "('also', 1)\n",
      "('suffers', 3)\n",
      "('milky', 1)\n",
      "('rageth', 1)\n",
      "('address', 1)\n",
      "('stealing', 1)\n",
      "('stage', 2)\n",
      "('pillow', 1)\n",
      "('kite', 1)\n",
      "('pledge', 1)\n",
      "('die', 11)\n",
      "('treasury', 1)\n",
      "('became', 1)\n",
      "('resolve', 1)\n",
      "('amplify', 1)\n",
      "('neck', 3)\n",
      "('obedience', 5)\n",
      "('divide', 1)\n",
      "(\"cover'd\", 2)\n",
      "('captain', 14)\n",
      "('legitimate', 5)\n",
      "('perdu', 1)\n",
      "('revenue', 4)\n",
      "('covering', 1)\n",
      "('crow', 1)\n",
      "('entertain', 1)\n",
      "('la', 2)\n",
      "('shallow', 1)\n",
      "('advancement', 1)\n",
      "('joys', 1)\n",
      "('reading', 1)\n",
      "('licensed', 1)\n",
      "('executing', 1)\n",
      "('mutations', 1)\n",
      "(\"loop'd\", 1)\n",
      "('fitly', 2)\n",
      "('altogether', 3)\n",
      "('arrives', 1)\n",
      "(\"lord's\", 1)\n",
      "('labours', 2)\n",
      "('nobly', 1)\n",
      "('important', 1)\n",
      "('prince', 5)\n",
      "('sinning', 1)\n",
      "('lamentable', 1)\n",
      "('brown', 1)\n",
      "('dogs', 5)\n",
      "('wipe', 2)\n",
      "('loses', 2)\n",
      "('exeunt', 34)\n",
      "('coming', 9)\n",
      "(\"show'dst\", 1)\n",
      "('gone', 18)\n",
      "('meet', 8)\n",
      "('conduct', 2)\n",
      "('according', 1)\n",
      "('fields', 1)\n",
      "(\"'tis\", 53)\n",
      "('enguard', 1)\n",
      "('zo', 1)\n",
      "('unbutton', 1)\n",
      "('credit', 1)\n",
      "('belly', 2)\n",
      "('corrupter', 1)\n",
      "('forty', 1)\n",
      "('took', 7)\n",
      "('confine', 1)\n",
      "(\"for't\", 2)\n",
      "(\"vex'd\", 1)\n",
      "('many', 12)\n",
      "(\"restrain'd\", 2)\n",
      "('tie', 1)\n",
      "('fell', 4)\n",
      "('mood', 1)\n",
      "('portable', 1)\n",
      "('burn', 2)\n",
      "(\"man's\", 15)\n",
      "('war', 2)\n",
      "('swore', 1)\n",
      "('unconstant', 1)\n",
      "('rivals', 1)\n",
      "('marks', 1)\n",
      "('sentence', 1)\n",
      "(\"howl'd\", 1)\n",
      "('goest', 2)\n",
      "('deeper', 1)\n",
      "('pestilent', 1)\n",
      "('moulten', 1)\n",
      "('hand', 26)\n",
      "('fearfully', 1)\n",
      "('warmth', 1)\n",
      "('pant', 1)\n",
      "('mates', 1)\n",
      "('crimes', 2)\n",
      "('cub', 1)\n",
      "('stock', 1)\n",
      "('confer', 1)\n",
      "('sparrow', 1)\n",
      "('dwells', 1)\n",
      "('twenty', 6)\n",
      "('offended', 2)\n",
      "('impatience', 1)\n",
      "('sot', 1)\n",
      "('rumble', 1)\n",
      "('sumpter', 1)\n",
      "('noises', 1)\n",
      "('obey', 6)\n",
      "('sovereign', 1)\n",
      "('haunts', 1)\n",
      "(\"it's\", 2)\n",
      "('unruly', 1)\n",
      "('freer', 1)\n",
      "('towards', 4)\n",
      "('side', 6)\n",
      "('tent', 4)\n",
      "('waking', 1)\n",
      "('suns', 1)\n",
      "('commanded', 1)\n",
      "('comedy', 1)\n",
      "('vices', 3)\n",
      "(\"answer'd\", 1)\n",
      "('brain', 2)\n",
      "('wear', 9)\n",
      "('tart', 1)\n",
      "('condemn', 1)\n",
      "('like', 49)\n",
      "('breach', 1)\n",
      "('merited', 1)\n",
      "('relieve', 1)\n",
      "('starts', 1)\n",
      "('temper', 2)\n",
      "('reconciles', 1)\n",
      "('brewers', 1)\n",
      "('abuse', 1)\n",
      "('benison', 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs_pattern = 'data/*'\n",
    "outputs_prefix = 'outputs_spark/part'\n",
    "\n",
    "# Spark runner options\n",
    "beam_options = PipelineOptions([\n",
    "    '--runner=SparkRunner',           # This tells Beam to use the Spark runtime\n",
    "    '--spark_master=local[*]',        # using local Spark (Tells Spark to run only ON MY MACHINE and not a Spark cluster)\n",
    "    '--job_name=wordcount-sparklocal' ])\n",
    "\n",
    "with beam.Pipeline(options=beam_options) as pipeline:\n",
    "    word_counts = (\n",
    "        pipeline\n",
    "        | 'Read lines' >> beam.io.ReadFromText(inputs_pattern)\n",
    "        | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line))\n",
    "        \n",
    "        ##### MODIFICATION #####\n",
    "      \n",
    "        # Converts all words to lowercase to ensure that the same words with different cases are counted together. \n",
    "        # For example, 'The' and 'the' will be treated as the same word 'the'. Similary 'KING', 'King' and 'king' will be treated\n",
    "        # as the same word 'king'.\n",
    "        | 'Lowercase words' >> beam.Map(lambda word: word.lower())\n",
    "        \n",
    "        | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
    "        | 'Group and sum' >> beam.CombinePerKey(sum)\n",
    "    )\n",
    "\n",
    "    (\n",
    "        word_counts\n",
    "        | 'Format results' >> beam.Map(lambda wc: f'{wc}')\n",
    "        | 'Write results' >> beam.io.WriteToText(outputs_prefix)\n",
    "    )\n",
    "\n",
    "# Peek at output\n",
    "run('cat outputs_spark/part-* > outputs_spark/all_results.txt')\n",
    "# run('cat outputs_spark/part-* | head -n 50')\n",
    "run('head -n 200 {}-00000-of-*'.format(outputs_prefix))\n",
    "\n",
    "\n",
    "\n",
    "#### The data gets stored in the outputs_spark folder and we can explore the complete word counts(all_results.txt) there. \n",
    "#### The results are an exact match with that of the DirectRunner results. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Try Apache Beam - Python",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
